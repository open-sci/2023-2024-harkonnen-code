{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = ZipFile(\"chunk1.zip\").namelist()\n",
    "print(check[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe PeerExtractor:\n",
    "\n",
    "Gestisce l'estrazione e la decompressione dei file JSON dall'archivio ZIP.\n",
    "Contiene il metodo statico process_json_data per processare i dati JSON decompressi.\n",
    "\n",
    "#### Classe OciProcess:\n",
    "\n",
    "Inizializza e gestisce il dizionario di lookup per i codici OCI.\n",
    "Include metodi per calcolare il prossimo codice di lookup, aggiornare il dizionario di lookup, e convertire un DOI in un codice CI.\n",
    "#### Classe CSVWriter:\n",
    "\n",
    "Gestisce la scrittura dei dati degli articoli su file CSV.\n",
    "Utilizza la classe OciProcess per generare i codici OCI per ogni articolo.\n",
    "\n",
    "#### Metodo parse_date:\n",
    "\n",
    "Funzione di supporto per il parsing delle date.\n",
    "Questo approccio orientato agli oggetti migliora la modularità e la manutenibilità del codice, rendendo più facile l'estensione e la gestione delle singole componenti del processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import errno\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "\n",
    "LOOKUP_CSV = 'lookup.csv'\n",
    "CROSSREF_CODE = '020'\n",
    "\n",
    "class PeerExtractor:\n",
    "    def __init__(self, zip_filename, batch_size=10):\n",
    "        self.zip_filename = zip_filename\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def process_files(self, csv_writer, max_files=None):\n",
    "        with zipfile.ZipFile(self.zip_filename, 'r') as zip_file:\n",
    "            file_infos = [file_info for file_info in zip_file.infolist() if file_info.filename.endswith(\".json.gz\")]\n",
    "            \n",
    "            if max_files:\n",
    "                file_infos = file_infos[:max_files]\n",
    "            \n",
    "            for batch in self.batch(file_infos, self.batch_size):\n",
    "                peer_review_items = self.process_batch(zip_file, batch)\n",
    "                csv_writer.write_to_csv(peer_review_items)\n",
    "\n",
    "    def batch(self, iterable, n=1):\n",
    "        length = len(iterable)\n",
    "        for ndx in range(0, length, n):\n",
    "            yield iterable[ndx:min(ndx + n, length)]\n",
    "\n",
    "    def process_batch(self, zip_file, batch):\n",
    "        peer_review_items = []\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            futures = [executor.submit(self.process_file, zip_file, file_info) for file_info in batch]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batch\"):\n",
    "                peer_review_items.extend(future.result())\n",
    "        return peer_review_items\n",
    "\n",
    "    def process_file(self, zip_file, file_info):\n",
    "        with zip_file.open(file_info) as compressed_file:\n",
    "            compressed_data = compressed_file.read()\n",
    "            return self.process_json_data(compressed_data)\n",
    "\n",
    "    def process_json_data(self, compressed_data):\n",
    "        decompressed_data = gzip.decompress(compressed_data)\n",
    "        decoded_data = decompressed_data.decode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(decoded_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Errore di decodifica JSON per il seguente dato: \", decoded_data)\n",
    "            return []\n",
    "\n",
    "        if isinstance(json_data, dict) and 'items' in json_data:\n",
    "            items = json_data['items']\n",
    "        elif isinstance(json_data, list):\n",
    "            items = json_data\n",
    "        else:\n",
    "            print(\"Struttura JSON non riconosciuta: \", json_data)\n",
    "            return []\n",
    "\n",
    "        peer_review_items = [item for item in items if item.get('type') == 'peer-review']\n",
    "        return peer_review_items\n",
    "\n",
    "class OciProcess:\n",
    "    def __init__(self, lookup_csv=LOOKUP_CSV, crossref_code=CROSSREF_CODE):\n",
    "        self.lookup_code = 0\n",
    "        self.lookup_dic = {}\n",
    "        self.LOOKUP_CSV = lookup_csv\n",
    "        self.CROSSREF_CODE = crossref_code\n",
    "        self.init_lookup_dic()\n",
    "\n",
    "    def init_lookup_dic(self):\n",
    "        with open(self.LOOKUP_CSV, 'r', encoding='utf-8') as lookupcsv:\n",
    "            lookupcsv_reader = csv.DictReader(lookupcsv)\n",
    "            code = -1\n",
    "            for row in lookupcsv_reader:\n",
    "                if row['c'] not in self.lookup_dic:\n",
    "                    self.lookup_dic[row['c']] = row['code']\n",
    "                    code = int(row['code'])\n",
    "            self.lookup_code = code\n",
    "\n",
    "    def calc_next_lookup_code(self):\n",
    "        rem = self.lookup_code % 100\n",
    "        newcode = self.lookup_code + 1\n",
    "        if rem == 89:\n",
    "            newcode = (self.lookup_code // 100 + 1) * 100\n",
    "        self.lookup_code = newcode\n",
    "\n",
    "    def update_lookup(self, c):\n",
    "        if c not in self.lookup_dic:\n",
    "            self.calc_next_lookup_code()\n",
    "            code = str(self.lookup_code).zfill(2)\n",
    "            self.lookup_dic[c] = code\n",
    "            self.write_txtblock_on_csv(self.LOOKUP_CSV, '\\n\"%s\",\"%s\"' % (c, code))\n",
    "\n",
    "    def write_txtblock_on_csv(self, csv_path, block_txt):\n",
    "        directory = os.path.dirname(csv_path)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            try:\n",
    "                os.makedirs(directory)\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "        with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            csvfile.write(block_txt)\n",
    "\n",
    "    def convert_doi_to_ci(self, doi_str):\n",
    "        return self.CROSSREF_CODE + self.match_str_to_lookup(doi_str)\n",
    "\n",
    "    def match_str_to_lookup(self, str_val):\n",
    "        ci_str = \"\"\n",
    "        str_noprefix = str_val[3:]\n",
    "        for c in str_noprefix:\n",
    "            if c not in self.lookup_dic:\n",
    "                self.update_lookup(c)\n",
    "            ci_str += str(self.lookup_dic[c])\n",
    "        return ci_str\n",
    "\n",
    "class CSVWriter:\n",
    "    def __init__(self, output_filenames):\n",
    "        if isinstance(output_filenames, str):\n",
    "            self.output_filenames = [output_filenames]\n",
    "        else:\n",
    "            self.output_filenames = output_filenames\n",
    "\n",
    "    def write_to_csv(self, peer_review_items):\n",
    "        for output_filename in self.output_filenames:\n",
    "            with open(output_filename, 'a', newline='', encoding='utf-8') as output_file:\n",
    "                fieldnames = [\"oci\", \"citing_doi\", \"cited_doi\", \"citing_date\", \"citing_url\"]\n",
    "                writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "                if output_file.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "\n",
    "                oci_processor = OciProcess()\n",
    "\n",
    "                for element in peer_review_items:\n",
    "                    for i in element.get(\"relation\", {}).get(\"is-review-of\", []):\n",
    "                        doi_p = element[\"DOI\"]\n",
    "                        doi_a = i.get(\"id\", \"\")\n",
    "                        url_p = element[\"URL\"]\n",
    "                        citing_entity_local_id = oci_processor.convert_doi_to_ci(doi_p)\n",
    "                        cited_entity_local_id = oci_processor.convert_doi_to_ci(doi_a)\n",
    "                        oci = \"oci:\" + citing_entity_local_id + \"-\" + cited_entity_local_id\n",
    "                        date_peer_review = str(element[\"created\"][\"date-time\"])[:10]\n",
    "                        if doi_p and doi_a:\n",
    "                            writer.writerow({\n",
    "                                \"oci\": oci,\n",
    "                                \"citing_doi\": doi_p,\n",
    "                                \"cited_doi\": doi_a,\n",
    "                                \"citing_date\": date_peer_review,\n",
    "                                \"citing_url\": url_p\n",
    "                            })\n",
    "            print(\"peer items saved to\", output_filename)\n",
    "\n",
    "    def remove_duplicates(self, input_filename, output_filename):\n",
    "        df = pl.read_csv(input_filename)\n",
    "        df_unique = df.unique(subset=['oci'])\n",
    "        df_unique.write_csv(output_filename)\n",
    "        print(f\"Unique peer items saved to {output_filename}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process JSON.gz files in a ZIP and output to CSV.\")\n",
    "    parser.add_argument(\"zip_filename\", help=\"The input ZIP file containing JSON.gz files.\")\n",
    "    parser.add_argument(\"output_filenames\", help=\"The output CSV file(s).\", nargs='+')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10, help=\"Number of files to process in each batch.\")\n",
    "    parser.add_argument(\"--max_files\", type=int, help=\"Maximum number of files to process.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    csv_writer = CSVWriter(args.output_filenames)\n",
    "    article_processor = PeerExtractor(args.zip_filename, args.batch_size)\n",
    "    article_processor.process_files(csv_writer, args.max_files)\n",
    "\n",
    "    for output_filename in args.output_filenames:\n",
    "        unique_output_filename = output_filename.replace(\".csv\", \"_unique.csv\")\n",
    "        csv_writer.remove_duplicates(output_filename, unique_output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NonPeerExtractor:\n",
    "    def __init__(self, zip_filename, batch_size=10):\n",
    "        self.zip_filename = zip_filename\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def process_files(self, csv_writer, max_files=None):\n",
    "        with zipfile.ZipFile(self.zip_filename, 'r') as zip_file:\n",
    "            file_infos = [file_info for file_info in zip_file.infolist() if file_info.filename.endswith(\".json.gz\")]\n",
    "            \n",
    "            if max_files:\n",
    "                file_infos = file_infos[:max_files]\n",
    "            \n",
    "            for batch in self.batch(file_infos, self.batch_size):\n",
    "                non_peer_review_items = self.process_batch(zip_file, batch)\n",
    "                csv_writer.write_to_csv(non_peer_review_items)\n",
    "\n",
    "    def batch(self, iterable, n=1):\n",
    "        length = len(iterable)\n",
    "        for ndx in range(0, length, n):\n",
    "            yield iterable[ndx:min(ndx + n, length)]\n",
    "\n",
    "    def process_batch(self, zip_file, batch):\n",
    "        non_peer_review_items = []\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            futures = [executor.submit(self.process_file, zip_file, file_info) for file_info in batch]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batch\"):\n",
    "                non_peer_review_items.extend(future.result())\n",
    "        return non_peer_review_items\n",
    "\n",
    "    def process_file(self, zip_file, file_info):\n",
    "        with zip_file.open(file_info) as compressed_file:\n",
    "            compressed_data = compressed_file.read()\n",
    "            return self.process_json_data(compressed_data)\n",
    "\n",
    "    def process_json_data(self, compressed_data):\n",
    "        decompressed_data = gzip.decompress(compressed_data)\n",
    "        decoded_data = decompressed_data.decode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(decoded_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Errore di decodifica JSON per il seguente dato: \", decoded_data)\n",
    "            return []\n",
    "\n",
    "        if isinstance(json_data, dict) and 'items' in json_data:\n",
    "            items = json_data['items']\n",
    "        elif isinstance(json_data, list):\n",
    "            items = json_data\n",
    "        else:\n",
    "            print(\"Struttura JSON non riconosciuta: \", json_data)\n",
    "            return []\n",
    "\n",
    "        non_peer_review_items = [item for item in items if item.get('type') != 'peer-review']\n",
    "        return non_peer_review_items\n",
    "\n",
    "class CSVWriter:\n",
    "    def __init__(self, output_filenames):\n",
    "        if isinstance(output_filenames, str):\n",
    "            self.output_filenames = [output_filenames]\n",
    "        else:\n",
    "            self.output_filenames = output_filenames\n",
    "        self.header_written = False\n",
    "\n",
    "    def write_to_csv(self, non_peer_review_items):\n",
    "        for output_filename in self.output_filenames:\n",
    "            with open(output_filename, 'a', newline='', encoding='utf-8') as output_file:\n",
    "                fieldnames = ['cited_doi', 'cited_url', 'cited_issn', 'cited_venue', 'cited_date']\n",
    "                writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "                if output_file.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "\n",
    "                for element in non_peer_review_items:\n",
    "                    doi_a = element.get(\"DOI\")\n",
    "                    url_a = element.get(\"URL\")\n",
    "                    date_non_peer_review = str(element.get(\"created\", {}).get(\"date-time\", \"\"))[:10]\n",
    "                    issn = ', '.join(element.get('ISSN', []))\n",
    "                    container_title = ', '.join(element.get('container-title', []))\n",
    "                    if doi_a and url_a:\n",
    "                        writer.writerow({\n",
    "                            \"cited_doi\": doi_a,\n",
    "                            \"cited_url\": url_a,\n",
    "                            \"cited_issn\": issn,\n",
    "                            \"cited_venue\": container_title,\n",
    "                            \"cited_date\": date_non_peer_review\n",
    "                        })\n",
    "            print(\"Batch saved to\", output_filename)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process JSON.gz files in a ZIP and output to CSV.\")\n",
    "    parser.add_argument(\"zip_filename\", help=\"The input ZIP file containing JSON.gz files.\")\n",
    "    parser.add_argument(\"output_filenames\", help=\"The output CSV file(s).\", nargs='+')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10, help=\"Number of files to process in each batch.\")\n",
    "    parser.add_argument(\"--max_files\", type=int, help=\"Maximum number of files to process.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    csv_writer = CSVWriter(args.output_filenames)\n",
    "    article_processor = NonPeerExtractor(args.zip_filename, args.batch_size)\n",
    "    article_processor.process_files(csv_writer, args.max_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "class Filter:\n",
    "    def __init__(self, peer_review_paths, non_peer_review_paths, output_path, column_to_join=\"cited_doi\"):\n",
    "        self.peer_review_paths = peer_review_paths\n",
    "        self.non_peer_review_paths = non_peer_review_paths\n",
    "        self.output_path = output_path\n",
    "        self.column_to_join = column_to_join\n",
    "\n",
    "    def read_and_concatenate_dataframes(self, paths):\n",
    "        dataframes = [pl.read_csv(path) for path in paths]\n",
    "        concatenated_df = pl.concat(dataframes)\n",
    "        return concatenated_df\n",
    "\n",
    "    def validate_dataframes(self, df1, df2):\n",
    "        if self.column_to_join not in df1.columns or self.column_to_join not in df2.columns:\n",
    "            raise ValueError(f\"Column {self.column_to_join} is not present in both DataFrames.\")\n",
    "\n",
    "    def join_dataframes(self, df1, df2):\n",
    "        joined_df = df1.join(df2, on=self.column_to_join, how=\"inner\")\n",
    "        return joined_df\n",
    "\n",
    "    def add_provenance(self, df):\n",
    "        # Define provenance information\n",
    "        prov_agent_url = \"https://academictorrents.com/details/d9e554f4f0c3047d9f49e448a7004f7aa1701b69\"\n",
    "        source_url = \"https://doi.org/10.13003/8wx5k\"\n",
    "        timezone = pytz.timezone(\"UTC\")\n",
    "        current_timestamp = datetime.now(timezone).strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "\n",
    "        # Add provenance columns\n",
    "        df = df.with_columns([\n",
    "            pl.lit(prov_agent_url).alias('prov_agent'),\n",
    "            pl.lit(source_url).alias('source'),\n",
    "            pl.lit(current_timestamp).alias('prov_date')\n",
    "        ])\n",
    "        return df\n",
    "\n",
    "    def save_csv(self, df, output_csv):\n",
    "        df.write_csv(output_csv)\n",
    "        print(f\"Updated CSV saved as {output_csv}\")\n",
    "\n",
    "class Delta:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_years(date_str):\n",
    "        return len(date_str) >= 4\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_months(date_str):\n",
    "        return len(date_str) >= 7\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_days(date_str):\n",
    "        return len(date_str) >= 10\n",
    "\n",
    "    @staticmethod\n",
    "    def get_iso8601_duration(delta):\n",
    "        duration = \"P\"\n",
    "        if delta.years:\n",
    "            duration += f\"{abs(delta.years)}Y\"\n",
    "        if delta.months:\n",
    "            duration += f\"{abs(delta.months)}M\"\n",
    "        if delta.days:\n",
    "            duration += f\"{abs(delta.days)}D\"\n",
    "        if duration == \"P\":\n",
    "            duration += \"0D\"\n",
    "        return duration\n",
    "\n",
    "    def calculate_date_difference(self, citing_date, cited_date):\n",
    "        default_date = datetime(1900, 1, 1)\n",
    "        \n",
    "        if self.contains_years(cited_date) and self.contains_years(citing_date):\n",
    "            citing_pub_datetime = parse(citing_date[:10], default=default_date)\n",
    "            cited_pub_datetime = parse(cited_date[:10], default=default_date)\n",
    "            delta = relativedelta(citing_pub_datetime, cited_pub_datetime)\n",
    "\n",
    "            iso8601_duration = self.get_iso8601_duration(delta)\n",
    "            if citing_pub_datetime < cited_pub_datetime: #controlla se il delta è negativo e mette il \"-\" davanti la P\n",
    "                iso8601_duration = f\"-{iso8601_duration}\"\n",
    "            return iso8601_duration\n",
    "        else:\n",
    "            return \"Invalid dates\"\n",
    "\n",
    "    def add_delta_column(self):\n",
    "        self.df = self.df.with_columns(\n",
    "            pl.struct([\"citing_date\", \"cited_date\"]).map_elements(\n",
    "                lambda x: self.calculate_date_difference(x[\"citing_date\"], x[\"cited_date\"]),\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias(\"time_span\")\n",
    "        )\n",
    "\n",
    "    def save_csv(self, output_csv):\n",
    "        self.df.write_csv(output_csv)\n",
    "        print(f\"CSV with Delta column saved as {output_csv}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Join peer review and non-peer review DataFrames and calculate delta.\")\n",
    "    parser.add_argument(\"--peer_review_paths\", help=\"The paths of the peer review DataFrames.\", nargs='+', required=True)\n",
    "    parser.add_argument(\"--non_peer_review_paths\", help=\"The paths of the non-peer review DataFrames.\", nargs='+', required=True)\n",
    "    parser.add_argument(\"--output_path\", help=\"The path of the output CSV file.\", required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_filter = Filter(args.peer_review_paths, args.non_peer_review_paths, args.output_path)\n",
    "    \n",
    "    concatenated_peer_df = data_filter.read_and_concatenate_dataframes(args.peer_review_paths)\n",
    "    concatenated_non_peer_df = data_filter.read_and_concatenate_dataframes(args.non_peer_review_paths)\n",
    "    \n",
    "    data_filter.validate_dataframes(concatenated_peer_df, concatenated_non_peer_df)\n",
    "    \n",
    "    joined_df = data_filter.join_dataframes(concatenated_peer_df, concatenated_non_peer_df)\n",
    "    \n",
    "    joined_df_with_provenance = data_filter.add_provenance(joined_df)\n",
    "    \n",
    "    delta_calculator = Delta(joined_df_with_provenance)\n",
    "    delta_calculator.add_delta_column()\n",
    "    \n",
    "    delta_calculator.save_csv(args.output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
