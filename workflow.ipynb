{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- funzioni come classi\n",
    "- chiamare da terminale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Peer Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cambiare nome chunk all'occorrenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import errno\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import polars as pl\n",
    "\n",
    "zip_filename = \"chunk6.zip\"\n",
    "\n",
    "def process_json_data(compressed_data):\n",
    "    decompressed_data = gzip.decompress(compressed_data) # decompression\n",
    "    decoded_data = decompressed_data.decode('utf-8') # decode as utf8\n",
    "    json_data = json.loads(decoded_data)\n",
    "    peer_review_items = [item for item in json_data['items'] if item.get('type') == 'peer-review'] # filtering elements\n",
    "    return peer_review_items\n",
    "\n",
    "output_filename = \"info_peer_review6.csv\"\n",
    "LOOKUP_CSV = 'lookup.csv'\n",
    "CROSSREF_CODE = '020'\n",
    "\n",
    "# handle oci\n",
    "\n",
    "class OciProcess:\n",
    "    def __init__(self):\n",
    "        self.lookup_code = 0\n",
    "        self.lookup_dic = {}\n",
    "        self.LOOKUP_CSV = 'lookup.csv'\n",
    "        self.CROSSREF_CODE = '020'\n",
    "        self.init_lookup_dic()  # Ensure lookup dictionary is initialized\n",
    "\n",
    "    def init_lookup_dic(self):\n",
    "        with open(self.LOOKUP_CSV, 'r', encoding='utf-8') as lookupcsv:\n",
    "            lookupcsv_reader = csv.DictReader(lookupcsv)\n",
    "            code = -1\n",
    "            for row in lookupcsv_reader:\n",
    "                if row['c'] not in self.lookup_dic:  # Avoid duplicates\n",
    "                    self.lookup_dic[row['c']] = row['code']\n",
    "                    code = int(row['code'])\n",
    "            self.lookup_code = code\n",
    "\n",
    "    def calc_next_lookup_code(self):\n",
    "        rem = self.lookup_code % 100\n",
    "        newcode = self.lookup_code + 1\n",
    "        if rem == 89:\n",
    "            newcode = (self.lookup_code // 100 + 1) * 100\n",
    "        self.lookup_code = newcode\n",
    "\n",
    "    def update_lookup(self, c):\n",
    "        if c not in self.lookup_dic:\n",
    "            self.calc_next_lookup_code()\n",
    "            code = str(self.lookup_code).zfill(2)  # Ensure code has at least 2 digits\n",
    "            self.lookup_dic[c] = code\n",
    "            self.write_txtblock_on_csv(self.LOOKUP_CSV, '\\n\"%s\",\"%s\"' % (c, code))\n",
    "\n",
    "    def check_make_dirs(self, filename):\n",
    "        directory = os.path.dirname(filename)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            try:\n",
    "                os.makedirs(directory)\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "    def write_txtblock_on_csv(self, csv_path, block_txt):\n",
    "        self.check_make_dirs(csv_path)\n",
    "        with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            csvfile.write(block_txt)\n",
    "\n",
    "    def convert_doi_to_ci(self, doi_str):\n",
    "        return self.CROSSREF_CODE + self.match_str_to_lookup(doi_str)\n",
    "\n",
    "    def match_str_to_lookup(self, str_val):\n",
    "        ci_str = \"\"\n",
    "        str_noprefix = str_val[3:]\n",
    "        for c in str_noprefix:\n",
    "            if c not in self.lookup_dic:\n",
    "                self.update_lookup(c)\n",
    "            ci_str += str(self.lookup_dic[c])\n",
    "        return ci_str\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_file:\n",
    "    with ThreadPoolExecutor() as executor: # parallelization\n",
    "        results = []\n",
    "        for file_info in zip_file.infolist(): # iteration over file in selected chunk (automatize later)\n",
    "            if file_info.filename.startswith(\"chunk6/\") and file_info.filename.endswith(\".json.gz\"): # additional check\n",
    "                with zip_file.open(file_info) as compressed_file: # open file in the zip archive\n",
    "                    compressed_data = compressed_file.read() # read data\n",
    "                    results.append(executor.submit(process_json_data, compressed_data)) # give task to ThreadPoolExecutor\n",
    "        \n",
    "        peer_review_items = []\n",
    "        for result in results:\n",
    "            peer_review_items.extend(result.result()) # retrieve results from executor\n",
    "\n",
    "# Write directly to CSV\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    fieldnames = [\"oci\", \"citing_doi\", \"cited_doi\", \"citing_date\", \"citing_url\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    uno = OciProcess()\n",
    "    due = OciProcess()\n",
    "\n",
    "    for element in peer_review_items:\n",
    "        for i in element.get(\"relation\", {}).get(\"is-review-of\", []):\n",
    "            doi_p = element[\"DOI\"]\n",
    "            doi_a = i.get(\"id\", \"\")\n",
    "            url_p = element[\"URL\"]\n",
    "            citing_entity_local_id = uno.convert_doi_to_ci(doi_p)\n",
    "            cited_entity_local_id = due.convert_doi_to_ci(doi_a)\n",
    "            oci = \"oci:\" + citing_entity_local_id + \"-\" + cited_entity_local_id\n",
    "            date_peer_review = str(element[\"created\"][\"date-time\"])[:10]\n",
    "            if doi_p and doi_a:\n",
    "                writer.writerow({\n",
    "                    \"oci\": oci,\n",
    "                    \"citing_doi\": doi_p,\n",
    "                    \"cited_doi\": doi_a,\n",
    "                    \"citing_date\": date_peer_review,\n",
    "                    \"citing_url\": url_p\n",
    "                })\n",
    "\n",
    "print(\"Info peer review items saved to\", output_filename)\n",
    "\n",
    "input_filename = \"info_peer_review6.csv\"\n",
    "output_filename_unique = \"info_peer_review6_unique.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pl.read_csv(input_filename)\n",
    "\n",
    "# Remove duplicate rows based on the \"OCI\" column\n",
    "df_unique = df.unique(subset=['OCI'])\n",
    "\n",
    "# Save the unique DataFrame back to a new CSV\n",
    "df_unique.write_csv(output_filename_unique)\n",
    "\n",
    "print(\"Unique info peer review items saved to\", output_filename_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concatenate Peer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_csv_files(csv_files_string, output_filename):\n",
    "    # Split the input string to get the list of CSV filenames\n",
    "    csv_files_list = csv_files_string.split(', ')\n",
    "\n",
    "    # Initialize an empty list to hold the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through the list of CSV filenames\n",
    "    for csv_file in csv_files_list:\n",
    "        # Read each CSV file into a dataframe and append it to the list\n",
    "        df = pd.read_csv(csv_file.strip())\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes\n",
    "    concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Write the concatenated dataframe to a new CSV file\n",
    "    concatenated_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Example usage\n",
    "csv_files_string = \"file1.csv, file2.csv, file3.csv\"\n",
    "output_filename = \"combined_output.csv\"\n",
    "concatenate_csv_files(csv_files_string, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Non peer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Funzione per estrarre informazioni rilevanti da un file JSON\n",
    "def extract_info_from_json(json_data):\n",
    "    info_list = []\n",
    "    for item in json_data['items']:\n",
    "        if 'peer-review' not in item.get('type', []):\n",
    "            info = {\n",
    "                'DOI': item.get('DOI', ''),\n",
    "                'URL': item.get('URL', ''),\n",
    "                'ISSN': ', '.join(item.get('ISSN', [])),\n",
    "                'container-title': ', '.join(item.get('container-title', [])),\n",
    "                'date-time': str(item.get('created', {}).get('date-time', ''))[:10]\n",
    "            }\n",
    "            info_list.append(info)\n",
    "    return info_list\n",
    "\n",
    "# Funzione per iterare sui file zippati e creare il CSV\n",
    "def process_zip_files(zip_filename, output_csv):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_file:\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['DOI', 'URL', 'ISSN', 'container-title', 'date-time']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for file_info in zip_file.infolist():\n",
    "                if file_info.filename.endswith(\".json.gz\"):\n",
    "                    with zip_file.open(file_info) as compressed_file:\n",
    "                        compressed_data = compressed_file.read()\n",
    "                        decompressed_data = gzip.decompress(compressed_data)\n",
    "                        json_data = json.loads(decompressed_data)\n",
    "                        info_list = extract_info_from_json(json_data)\n",
    "                        writer.writerows(info_list)\n",
    "\n",
    "# Utilizzo della funzione\n",
    "zip_filename = \"chunk14.zip\"\n",
    "output_csv = \"info_non_peer_review9.csv\"\n",
    "process_zip_files(zip_filename, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge tables e Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "\n",
    "input_csv = 'combined_file_final.csv'\n",
    "temp_csv = 'tempfile.csv'\n",
    "prov_agent_url = \"https://academictorrents.com/details/d9e554f4f0c3047d9f49e448a7004f7aa1701b69\"\n",
    "source_url = \"https://doi.org/10.13003/8wx5k\"\n",
    "\n",
    "# Define the timezone (UTC in this example)\n",
    "timezone = pytz.timezone(\"UTC\")\n",
    "# Get the current time with timezone information in the desired format\n",
    "current_timestamp = datetime.now(timezone).strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "\n",
    "try:\n",
    "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile, open(temp_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Read the header\n",
    "        header = next(reader)\n",
    "        # Add new column headers\n",
    "        header.extend(['prov_agent', 'source', 'prov_date'])\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Process each row\n",
    "        for row in reader:\n",
    "            row.extend([prov_agent_url, source_url, current_timestamp])\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # Replace the original file with the temporary file\n",
    "    os.replace(temp_csv, input_csv)\n",
    "    print(f\"Updated CSV saved as {input_csv}\")\n",
    "\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Failed to decode using utf-8 encoding: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. RDF creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, RDF, RDFS, XSD, URIRef, Literal\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "class PeerReview(object):\n",
    "\n",
    "    # predicates citation\n",
    "    __cito_base = \"http://purl.org/spar/cito/\"\n",
    "    _reviews = URIRef(__cito_base + \"reviews\") \n",
    "    _citation = URIRef(__cito_base + \"Citation\")\n",
    "    _has_citation_creation_date = URIRef(__cito_base + \"hasCitationCreationDate\")\n",
    "    _has_citation_time_span = URIRef(__cito_base + \"hasCitationTimeSpan\")\n",
    "    _has_citing_entity = URIRef(__cito_base + \"hasCitingEntity\")\n",
    "    _has_cited_entity = URIRef(__cito_base + \"hasCitedEntity\")\n",
    "    _has_citation_characterization = URIRef(__cito_base + \"hasCitationCharacterisation\")\n",
    "\n",
    "    # predicates provenance\n",
    "    \n",
    "    __prov_base = \"http://www.w3.org/ns/prov#\"\n",
    "    _was_attributed_to = URIRef(__prov_base + \"wasAttributedTo\")\n",
    "    _had_primary_source = URIRef(__prov_base + \"hadPrimarySource\")\n",
    "    _generated_at_time = URIRef(__prov_base + \"generatedAtTime\")\n",
    "\n",
    "    # init\n",
    "\n",
    "    def __init__(self, oci, citing_url=None, cited_url=None, timespan=None, citing_date=None, prov_agent_url=None, source=None, prov_date=None):\n",
    "        self.oci = oci[4:]\n",
    "        self.citing_url = citing_url\n",
    "        self.citing_date = citing_date\n",
    "        self.cited_url = cited_url\n",
    "        self.timespan = timespan\n",
    "        self.prov_agent_url = prov_agent_url\n",
    "        self.source = source\n",
    "        self.prov_date = prov_date\n",
    "\n",
    "\n",
    "    def get_peer_review_rdf(self, baseurl, include_data=True, include_prov=True):\n",
    "        peer_review_graph = Graph()\n",
    "\n",
    "        citation_corpus_id = \"ci/\" + self.oci\n",
    "        citation = URIRef(baseurl + citation_corpus_id)\n",
    "\n",
    "        if include_data:\n",
    "            citing_br = URIRef(self.citing_url)\n",
    "            cited_br = URIRef(self.cited_url)\n",
    "\n",
    "            peer_review_graph.add((citation, RDF.type, self.__citation))\n",
    "            # ho rimosso le robe di self citation.\n",
    "            #statement of charaacherization\n",
    "            peer_review_graph.add((citation,self._has_citation_characterization, self._reviews))\n",
    "\n",
    "            peer_review_graph.add((citation, self.__has_citing_entity, citing_br))\n",
    "            peer_review_graph.add((citation, self.__has_cited_entity, cited_br))\n",
    "\n",
    "            if self.citing_date is not None:\n",
    "                if PeerReview.contains_days(self.citing_date):\n",
    "                    xsd_type = XSD.date\n",
    "                elif PeerReview.contains_months(self.citing_date):\n",
    "                    xsd_type = XSD.gYearMonth\n",
    "                else:\n",
    "                    xsd_type = XSD.gYear\n",
    "\n",
    "                peer_review_graph.add((citation, self.__has_citation_creation_date,\n",
    "                                    Literal(self.citing_date, datatype=xsd_type, normalize=False)))\n",
    "                if self.timespan is not None:\n",
    "                    peer_review_graph.add((citation, self.__has_citation_time_span,\n",
    "                                        Literal(self.timespan, datatype=XSD.duration)))\n",
    "                    \n",
    "\n",
    "        if include_prov:\n",
    "            peer_review_graph.add((citation, self.__was_attributed_to, URIRef(self.prov_agent_url)))\n",
    "            peer_review_graph.add((citation, self.__had_primary_source, URIRef(self.source)))\n",
    "            peer_review_graph.add((citation, self.__generated_at_time, Literal(self.prov_date, datatype=XSD.dateTime)))\n",
    "\n",
    "        return peer_review_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def contains_years(date):\n",
    "        return date is not None and len(date) >= 4\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_months(date):\n",
    "        return date is not None and len(date) >= 7\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_days(date):\n",
    "        return date is not None and len(date) >= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "from argparse import ArgumentParser\n",
    "# from citation import Citation\n",
    "from rdflib.namespace import RDF, RDFS, SKOS\n",
    "import csv\n",
    "import urllib\n",
    "from io import StringIO\n",
    "\n",
    "# Function to populate RDF data from CSV input\n",
    "import csv\n",
    "from io import StringIO\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "from rdflib import XSD, Literal\n",
    "\n",
    "\n",
    "def populate_data(csv_file, output_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "\n",
    "\n",
    "        for row in reader:\n",
    "            oci = row['oci']\n",
    "            citing_url = row['citing_url'] \n",
    "            cited_url = row['cited_url']\n",
    "            citing_date = row['citing_date'] \n",
    "            timespan = row['timespan']\n",
    "\n",
    "            citation = PeerReview(oci,\n",
    "                                  citing_url=citing_url,\n",
    "                                  cited_url=cited_url,\n",
    "                                  timespan=timespan,\n",
    "                                  citing_date=citing_date)\n",
    "\n",
    "            g = citation.get_peer_review_rdf(BASE_URL, include_data=INCLUDE_DATA, include_prov=INCLUDE_PROV)\n",
    "\n",
    "            # Open output file in append mode outside the inner loop\n",
    "            with open(output_file, 'a', newline='') as f:\n",
    "                f.write(g.serialize(format='nt'))\n",
    "\n",
    "\n",
    "\n",
    " # Function to populate provenance RDF data from CSV input\n",
    "def populate_prov(csv_file, output_file):\n",
    "    block_txt = ''  # Initialize an empty string to store RDF data\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            oci = row['oci']\n",
    "            agent_url = row['prov_agent_url']\n",
    "            prov_source = row['source']\n",
    "            prov_date = row['prov_date']\n",
    "\n",
    "\n",
    "            citation = PeerReview(oci,\n",
    "                                  prov_agent_url=agent_url,\n",
    "                                  source=prov_source,\n",
    "                                  prov_date=prov_date)\n",
    "\n",
    "            g = citation.get_peer_review_rdf(BASE_URL, include_data=INCLUDE_DATA, include_prov=INCLUDE_PROV)\n",
    "            block_txt += g.serialize(format='nt')\n",
    "\n",
    "    if block_txt:  # Write to output file if block_txt is not empty\n",
    "        with open(output_file, 'a', newline='') as f:\n",
    "            f.write(block_txt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "INPUT_ROOT_DIR = \".\"\n",
    "OUTPUT_FILE = \"dataset.ttl\"\n",
    "BASE_URL = \"https://w3id.org/oc/index/coci/\"\n",
    "INCLUDE_DATA = True\n",
    "INCLUDE_PROV = False\n",
    "ENTRIES_PER_FILE = 100000\n",
    "#BUFFER = 100000\n",
    "OUTPUT_FILE = str(ENTRIES_PER_FILE)+'.ttl'\n",
    "\n",
    "\n",
    "agent_url = \"https://w3id.org/oc/index/coci/prov/pa/1\"\n",
    "\n",
    "prov_source = \"https://api.crossref.org/works/\"\n",
    "\n",
    "\n",
    "csv_file = 'fakeassbitch.csv'\n",
    "output_file = 'ciaociaociao.ttl'\n",
    "populate_data(csv_file, OUTPUT_FILE)\n",
    "# populate_prov(csv_file, OUTPUT_FILE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ISSN table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(\"matched_info17.csv\")\n",
    "df = df.fillna('')\n",
    "\n",
    "# Filter rows containing a comma in the 'issn' column\n",
    "comma_rows = df[df['issn'].str.contains(',')]\n",
    "\n",
    "# Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "comma_rows = comma_rows.copy()\n",
    "\n",
    "# Split the 'issn' column into two separate columns for comma rows\n",
    "comma_rows[['issn1', 'issn2']] = comma_rows['issn'].str.split(',', expand=True)\n",
    "\n",
    "# Group by the two ISSN values and count occurrences for comma rows\n",
    "grouped_df_comma = comma_rows.groupby(['issn1', 'issn2']).size().reset_index(name='count')\n",
    "\n",
    "# Add the 'title' column to the grouped DataFrame\n",
    "grouped_df_comma['title'] = comma_rows.groupby(['issn1', 'issn2'])['title'].apply(lambda x: ', '.join(set(x))).reset_index(drop=True)\n",
    "\n",
    "# Filter rows without a comma in the 'issn' column\n",
    "no_comma_rows = df[~df['issn'].str.contains(',')]\n",
    "\n",
    "# Update counter for rows without comma and create new rows if needed\n",
    "for index, row in no_comma_rows.iterrows():\n",
    "    if row['issn'] in grouped_df_comma['issn1'].values or row['issn'] in grouped_df_comma['issn2'].values:\n",
    "        for i, g_row in grouped_df_comma.iterrows():\n",
    "            if row['issn'] == g_row['issn1'] or row['issn'] == g_row['issn2']:\n",
    "                grouped_df_comma.at[i, 'count'] += 1\n",
    "                break\n",
    "    else:\n",
    "        # Check if the ISSN is already in comma_rows to avoid duplicating titles\n",
    "        if row['issn'] in comma_rows['issn'].values:\n",
    "            title = comma_rows[comma_rows['issn'] == row['issn']]['title'].iloc[0]\n",
    "        else:\n",
    "            title = row['title']\n",
    "        # Check if the ISSN group already has the title appended\n",
    "        if (row['issn'], None) in zip(grouped_df_comma['issn1'], grouped_df_comma['issn2']):\n",
    "            continue\n",
    "        new_row = pd.DataFrame({'issn1': [row['issn']], 'issn2': [None], 'count': [1], 'title': [title]})\n",
    "        grouped_df_comma = pd.concat([grouped_df_comma, new_row], ignore_index=True)\n",
    "\n",
    "# Save the updated grouped data to a new CSV file\n",
    "grouped_df_comma.to_csv(\"grouped_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Meta counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to extract doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = 'csv_openalex.zip'\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_file_path = 'meta_doi.csv'\n",
    "\n",
    "# Clear the output file by opening it in write mode\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('DOI\\n')\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Iterate through the files in the zip\n",
    "    for file_name in zip_ref.namelist():\n",
    "        # Ensure the file is a CSV\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Open the CSV file from the zip\n",
    "            with zip_ref.open(file_name) as csv_file:\n",
    "                # Process the CSV file in chunks\n",
    "                for chunk in pd.read_csv(io.TextIOWrapper(csv_file, encoding='utf-8'), chunksize=10000):\n",
    "                    # Initialize an empty list to hold the DOI values for this chunk\n",
    "                    chunk_dois = []\n",
    "                    \n",
    "                    # Extract DOI values from the first column\n",
    "                    first_column_values = chunk.iloc[:, 0].tolist()\n",
    "                    for i in first_column_values:\n",
    "                        if 'doi:' in i:\n",
    "                            # Find the index of 'doi:'\n",
    "                            start_index = i.find('doi:') + len('doi:')\n",
    "                            # Find the index of the first blank space after 'doi:'\n",
    "                            end_index = i.find(' ', start_index)\n",
    "                            # If no space is found, use the length of the string\n",
    "                            if end_index == -1:\n",
    "                                end_index = len(i)\n",
    "                            # Extract the substring starting from the character after 'doi:' up to the first blank space\n",
    "                            doi = i[start_index:end_index]\n",
    "                            # If the DOI contains a comma, enclose it in quotes\n",
    "                            if ',' in doi:\n",
    "                                doi = f'\"{doi}\"'\n",
    "                            # Append the extracted DOI to the list for this chunk\n",
    "                            chunk_dois.append(doi)\n",
    "                    \n",
    "                    # Write the extracted DOIs to the output CSV file with utf-8 encoding\n",
    "                    with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "                        for doi in chunk_dois:\n",
    "                            output_file.write(f'{doi}\\n')\n",
    "\n",
    "print('file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to count lines in output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count line in meta-doi\n",
    "\n",
    "import csv\n",
    "\n",
    "def count_rows_in_csv(file_path):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        row_count = sum(1 for row in reader)\n",
    "    return row_count\n",
    "\n",
    "# Example usage\n",
    "file_path_2 = 'meta_doi.csv'\n",
    "print(f'The number of dois in Meta is: {count_rows_in_csv(file_path_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creates the two lists of peer reviews and articles reviewd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the input and output files\n",
    "csv_file_path = 'combined_file_final.csv'\n",
    "meta_file_path = 'meta_doi.csv'\n",
    "meta_peer_file_path = 'meta_peer.csv'\n",
    "meta_article_file_path = 'meta_article.csv'\n",
    "\n",
    "# Load 'combined_file_final.csv' into a DataFrame\n",
    "combined_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create sets from the columns 'DOI_peer' and 'DOI_article'\n",
    "doi_peer_set = set(combined_df['DOI_peer'])\n",
    "doi_article_set = set(combined_df['DOI_article'])\n",
    "\n",
    "# Initialize the output CSV files with headers\n",
    "with open(meta_peer_file_path, 'w', encoding='utf-8') as meta_peer_file:\n",
    "    meta_peer_file.write('DOI\\n')\n",
    "with open(meta_article_file_path, 'w', encoding='utf-8') as meta_article_file:\n",
    "    meta_article_file.write('DOI\\n')\n",
    "\n",
    "# Read 'meta_doi.csv' in chunks and process each chunk\n",
    "chunk_size = 10000\n",
    "for chunk in pd.read_csv(meta_file_path, chunksize=chunk_size):\n",
    "    meta_peer_list = []\n",
    "    meta_article_list = []\n",
    "    \n",
    "    # Iterate over each DOI in the chunk\n",
    "    for doi in chunk.iloc[:, 0]:  # Assuming the DOI is in the first column\n",
    "        if doi in doi_peer_set:\n",
    "            meta_peer_list.append(doi)\n",
    "        if doi in doi_article_set:\n",
    "            meta_article_list.append(doi)\n",
    "    \n",
    "    # Append the results to the respective output files\n",
    "    with open(meta_peer_file_path, 'a', encoding='utf-8') as meta_peer_file:\n",
    "        for peer_doi in meta_peer_list:\n",
    "            meta_peer_file.write(f'{peer_doi}\\n')\n",
    "    \n",
    "    with open(meta_article_file_path, 'a', encoding='utf-8') as meta_article_file:\n",
    "        for article_doi in meta_article_list:\n",
    "            meta_article_file.write(f'{article_doi}\\n')\n",
    "\n",
    "print('files created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count rows in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the output files\n",
    "meta_peer_file_path = 'meta_peer.csv'\n",
    "meta_article_file_path = 'meta_article.csv'\n",
    "\n",
    "# Function to count the number of rows in a CSV file\n",
    "def count_rows(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Skip the header line\n",
    "        next(file)\n",
    "        # Count the remaining lines\n",
    "        row_count = sum(1 for row in file)\n",
    "    return row_count\n",
    "\n",
    "# Count the rows in each output file\n",
    "meta_peer_row_count = count_rows(meta_peer_file_path)\n",
    "meta_article_row_count = count_rows(meta_article_file_path)\n",
    "\n",
    "# Print the results\n",
    "print(f'Number of rows in {meta_peer_file_path}: {meta_peer_row_count}')\n",
    "print(f'Number of rows in {meta_article_file_path}: {meta_article_row_count}')\n",
    "\n",
    "\n",
    "#sistemare per non prendere doppioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_duplicates_and_save(input_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Save the cleaned DataFrame back to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"Duplicates removed and cleaned file saved successfully!\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'meta_article.csv'\n",
    "input_file_count = count_rows(input_file)\n",
    "\n",
    "print(f'Number of rows in {input_file}: {input_file_count}')\n",
    "\n",
    "output_file = 'meta_article_cleaned.csv'\n",
    "drop_duplicates_and_save(input_file, output_file)\n",
    "output_file_count = count_rows(output_file)\n",
    "\n",
    "print(f'Number of rows in {output_file}: {output_file_count}') \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'meta_peer.csv'\n",
    "input_file_count = count_rows(input_file)\n",
    "\n",
    "print(f'Number of rows in {input_file}: {input_file_count}')\n",
    "\n",
    "output_file = 'meta_peer_cleaned.csv'\n",
    "drop_duplicates_and_save(input_file, output_file)\n",
    "output_file_count = count_rows(output_file)\n",
    "\n",
    "print(f'Number of rows in {output_file}: {output_file_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
