{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformare DIRETTAMENTE il json di crossref in csv utile alla nostra nobile causa\n",
    "#includere anche la strasformazione on fly dei doi in oci\n",
    "\n",
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import errno\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "zip_filename = \"chunk14.zip\"\n",
    "\n",
    "def process_json_data(compressed_data):\n",
    "    decompressed_data = gzip.decompress(compressed_data) #decompression\n",
    "    decoded_data = decompressed_data.decode('utf-8') #decode as utf8\n",
    "    json_data = json.loads(decoded_data)\n",
    "    peer_review_items = [item for item in json_data['items'] if item.get('type') == 'peer-review'] #fltering elements\n",
    "    return peer_review_items\n",
    "\n",
    "files = [\"peer_review_items11.json\"]\n",
    "output_filenames = [\"provina-oci.csv\"]\n",
    "LOOKUP_CSV = 'lookup.csv'\n",
    "CROSSREF_CODE = '020'\n",
    "\n",
    "def parse_date(date_parts):\n",
    "    if date_parts:\n",
    "        try:\n",
    "            return \"-\".join(map(str, date_parts))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "#handle oci\n",
    "class OciProcess:\n",
    "    def _init_(self):\n",
    "        self.lookup_code = 0\n",
    "        self.lookup_dic = {}\n",
    "        self.LOOKUP_CSV = 'lookup.csv'\n",
    "        self.CROSSREF_CODE = '020'\n",
    "          \n",
    "    def init_lookup_dic(self):\n",
    "        with open(self.LOOKUP_CSV,'r') as lookupcsv:\n",
    "            lookupcsv_reader = csv.DictReader(lookupcsv)\n",
    "            code = -1\n",
    "            for row in lookupcsv_reader:\n",
    "                self.lookup_dic[row['c']] = row['code']\n",
    "                code = int(row['code'])\n",
    "            #last code used\n",
    "            self.lookup_code = code\n",
    "    \n",
    "    def calc_next_lookup_code(self):\n",
    "        rem = self.lookup_code % 100\n",
    "        newcode = self.lookup_code + 1\n",
    "        if (rem == 89):\n",
    "            newcode = newcode * 10\n",
    "        self.lookup_code = newcode\n",
    "\n",
    "    def update_lookup(self, c):\n",
    "        if c not in self.lookup_dic:\n",
    "            #define the code following the 9 rule ...\n",
    "            self.calc_next_lookup_code()\n",
    "            code = self.lookup_code\n",
    "            self.lookup_dic[c] = code\n",
    "            self.write_txtblock_on_csv(self.LOOKUP_CSV, '\\n\"%s\",\"%s\"'%(c, code))\n",
    "    \n",
    "    def check_make_dirs(self, filename):\n",
    "        directory = os.path.dirname(filename)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            try:\n",
    "                os.makedirs(directory)\n",
    "            except OSError as exc:\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "    \n",
    "    def write_txtblock_on_csv(self, csv_path, block_txt):\n",
    "        self.check_make_dirs(csv_path)\n",
    "        with open(csv_path, 'a', newline='') as csvfile:\n",
    "            csvfile.write(block_txt)\n",
    "\n",
    "    def convert_doi_to_ci(self, doi_str):\n",
    "        CROSSREF_CODE = '020'\n",
    "        return self.CROSSREF_CODE + self.match_str_to_lookup(doi_str)\n",
    "    \n",
    "    def match_str_to_lookup(self, str_val):\n",
    "        ci_str = \"\"\n",
    "        str_noprefix = str_val[3:]\n",
    "        for c in str_noprefix:\n",
    "            if c not in self.lookup_dic:\n",
    "                self.update_lookup(c)\n",
    "            ci_str = ci_str + str(self.lookup_dic[c])\n",
    "        return ci_str\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_file:\n",
    "    with ThreadPoolExecutor() as executor: #parallelization\n",
    "        results = []\n",
    "        for file_info in zip_file.infolist(): #iteration over file in selected chunk (automatize later)\n",
    "            if file_info.filename.startswith(\"chunk14/\") and file_info.filename.endswith(\".json.gz\"): #additional check\n",
    "                with zip_file.open(file_info) as compressed_file: #open file in the zip archive\n",
    "                    compressed_data = compressed_file.read() #read data\n",
    "                    results.append(executor.submit(process_json_data, compressed_data)) #give task to threadpoolexecutor\n",
    "        \n",
    "        peer_review_items = []\n",
    "        for result in results:\n",
    "            peer_review_items.extend(result.result())   #retrieve results from executor\n",
    "\n",
    "# Write directly to CSV\n",
    "for output_filename in output_filenames:\n",
    "    with open(output_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        fieldnames = [\"OCI\", \"DOI_peer\", \"DOI_article\", \"date_peer_review\", \"author_given_name\", \"author_family_name\", \"URL_peer_review\"]\n",
    "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        uno = OciProcess()\n",
    "        due = OciProcess()\n",
    "\n",
    "        for element in peer_review_items:\n",
    "            for i in element.get(\"relation\", {}).get(\"is-review-of\"):\n",
    "                doi_p = element[\"DOI\"]\n",
    "                doi_a = i.get(\"id\", \"\")\n",
    "                url_p = element[\"URL\"]\n",
    "                citing_entity_local_id = uno.convert_doi_to_ci(doi_p)\n",
    "                cited_entity_local_id = due.convert_doi_to_ci(doi_a)\n",
    "                oci = \"oci:\" + citing_entity_local_id + \"-\" + cited_entity_local_id\n",
    "                date_peer_review = element.get(\"created\", {}).get(\"date-parts\", [[]])[0]\n",
    "                author_info = element.get(\"editor\", [{}])[0]\n",
    "                given_name = author_info.get(\"given\", \"\")\n",
    "                family_name = author_info.get(\"family\", \"\")\n",
    "                if doi_p and doi_a:\n",
    "                    writer.writerow({\n",
    "                        \"OCI\": oci,\n",
    "                        \"DOI_peer\": doi_p,\n",
    "                        \"DOI_article\": doi_a,\n",
    "                        \"date_peer_review\": parse_date(date_peer_review),\n",
    "                        \"author_given_name\": given_name,\n",
    "                        \"author_family_name\": family_name,\n",
    "                        \"URL_peer_review\": url_p\n",
    "                    })\n",
    "\n",
    "    print(\"URL pairs from peer review items saved to\", output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterazione sui file zippati e creazione di un CSV con le informazioni delle chiavi che ci interessano senza distinzione di pubblicazione (escludiamo solo le peer review):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Funzione per estrarre informazioni rilevanti da un file JSON\n",
    "def extract_info_from_json(json_data):\n",
    "    info_list = []\n",
    "    for item in json_data['items']:\n",
    "        if 'peer-review' not in item.get('type', []):\n",
    "            info = {\n",
    "                'DOI': item.get('DOI', ''),\n",
    "                'URL': item.get('URL', ''),\n",
    "                'ISSN': ', '.join(item.get('ISSN', [])),\n",
    "                'container-title': ', '.join(item.get('container-title', [])),\n",
    "                'date-time': str(item.get('created', {}).get('date-time', ''))[:10]\n",
    "            }\n",
    "            info_list.append(info)\n",
    "    return info_list\n",
    "\n",
    "# Funzione per iterare sui file zippati e creare il CSV\n",
    "def process_zip_files(zip_filename, output_csv):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_file:\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['DOI', 'URL', 'ISSN', 'container-title', 'date-time']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for file_info in zip_file.infolist():\n",
    "                if file_info.filename.endswith(\".json.gz\"):\n",
    "                    with zip_file.open(file_info) as compressed_file:\n",
    "                        compressed_data = compressed_file.read()\n",
    "                        decompressed_data = gzip.decompress(compressed_data)\n",
    "                        json_data = json.loads(decompressed_data)\n",
    "                        info_list = extract_info_from_json(json_data)\n",
    "                        writer.writerows(info_list)\n",
    "\n",
    "# Utilizzo della funzione\n",
    "zip_filename = \"chunk9.zip\"\n",
    "output_csv = \"info_non_peer_review9.csv\"\n",
    "process_zip_files(zip_filename, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterazione sul CSV per trovare i match con i DOI in combined_file_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Funzione per trovare i match con i DOI nel CSV\n",
    "def find_matches_in_csv(input_csv, match_csv, output_csv):\n",
    "    dois_to_match = set() # Set per memorizzare i DOI da cercare\n",
    "    with open(match_csv, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            dois_to_match.add(row['DOI_article'])\n",
    "    \n",
    "    matched_info = [] # Lista per memorizzare le informazioni corrispondenti\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['DOI'] in dois_to_match:\n",
    "                matched_info.append(row)\n",
    "    \n",
    "    # Scrivere le informazioni corrispondenti in un nuovo CSV\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['DOI', 'URL', 'ISSN', 'container-title', 'timestamp']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(matched_info)\n",
    "\n",
    "# Utilizzo della funzione\n",
    "input_csv = \"info_non_peer_review.csv\"\n",
    "match_csv = \"combined_file_final.csv\"\n",
    "output_csv = \"matched_info10.csv\"\n",
    "find_matches_in_csv(input_csv, match_csv, output_csv)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
